{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datas(file_path:str):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def get_training_and_validation_datas(df: pd.DataFrame, training_size = 0.8):\n",
    "    data_size = df.shape[0]\n",
    "    indexes = [i for i in range(data_size)]\n",
    "    training_index = np.random.choice(indexes,int(data_size*training_size))\n",
    "    validation_index = [i for i in range(data_size) if not i in training_index]\n",
    "    training_input = [df['Text'].iloc[index] for index in training_index]\n",
    "    training_output = [df['Sentiment'].iloc[index] for index in training_index]\n",
    "    validation_input = [df['Text'].iloc[index] for index in validation_index]\n",
    "    validation_output = [df['Sentiment'].iloc[index] for index in validation_index]\n",
    "    return training_input, training_output, validation_input, validation_output\n",
    "\n",
    "def get_TP_TN_FP_FN(computed_output, ground_truth, positive_label):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    for i in range(len(computed_output)):\n",
    "        if computed_output[i] == positive_label:\n",
    "            if ground_truth[i] == positive_label:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            if ground_truth[i] == positive_label:\n",
    "                FN += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def get_accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def get_precision(TP, TN, FP, FN):\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def get_recall(TP, TN, FP, FN):\n",
    "    return TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸ—Ÿ Extragerea de caracteristici din text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bags_of_words(training_input, validation_input):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_features = vectorizer.fit_transform(training_input)\n",
    "    validation_features = vectorizer.transform(validation_input)\n",
    "    return train_features, validation_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(training_input, validation_input, max_feats=50):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_feats)\n",
    "    train_features = vectorizer.fit_transform(training_input)\n",
    "    validation_features = vectorizer.transform(validation_input)\n",
    "    return train_features, validation_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alte caracteristici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\IrimieÅŸ\n",
      "[nltk_data]     Vasile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_stemmed(training_input, validation_input):\n",
    "    training_input_tokens = [ word_tokenize(text) for text in training_input ]\n",
    "    validation_input_tokens = [word_tokenize(text) for text in validation_input]\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    training_input_stemmed_words = [[stemmer.stem(word) for word in words] for words in training_input_tokens]\n",
    "    validation_input_stemmed_words = [[stemmer.stem(word) for word in words] for words in validation_input_tokens]\n",
    "\n",
    "    training_input_stemmed_sentences = [' '.join(words) for words in training_input_stemmed_words]\n",
    "    validation_input_stemmed_sentences = [' '.join(words) for words in validation_input_stemmed_words]\n",
    "    \n",
    "    return training_input_stemmed_sentences,validation_input_stemmed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "['We had requested two queen beds and got a room with 1 queen and 2 twins, we were advised that there were not any other rooms and could put the (2) beds together, which we did and it was no issue as far as sleeping.']\n",
      "After: \n",
      "['we had request two queen bed and got a room with 1 queen and 2 twin , we were advis that there were not ani other room and could put the ( 2 ) bed togeth , which we did and it wa no issu as far as sleep .']\n",
      "\n",
      "Before: \n",
      "['Waited over 40 minutes to use shower.']\n",
      "After: \n",
      "['wait over 40 minut to use shower .']\n"
     ]
    }
   ],
   "source": [
    "s1_initial = [\"We had requested two queen beds and got a room with 1 queen and 2 twins, we were advised that there were not any other rooms and could put the (2) beds together, which we did and it was no issue as far as sleeping.\"]\n",
    "s2_initial = [\"Waited over 40 minutes to use shower.\"]\n",
    "v1,v2 = get_stemmed(s1_initial,s2_initial)\n",
    "print(\"Before: \")\n",
    "print(s1_initial)\n",
    "print(\"After: \")\n",
    "print(v1)\n",
    "print()\n",
    "\n",
    "print(\"Before: \")\n",
    "print(s2_initial)\n",
    "print(\"After: \")\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\IrimieÅŸ\n",
      "[nltk_data]     Vasile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\IrimieÅŸ\n",
      "[nltk_data]     Vasile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_stop_words_removal(training_input, validation_input):\n",
    "    training_input_tokens = [ word_tokenize(text) for text in training_input ]\n",
    "    validation_input_tokens = [word_tokenize(text) for text in validation_input]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    training_input_words_without_stop = [[word for word in words if word not in stop_words] for words in training_input_tokens]\n",
    "    validation_input_words_without_stop = [[word for word in words if word not in stop_words] for words in validation_input_tokens]\n",
    "\n",
    "    training_input_without_stop = [' '.join(words) for words in training_input_words_without_stop]\n",
    "    validation_input_without_stop = [' '.join(words) for words in validation_input_words_without_stop]\n",
    "\n",
    "    return training_input_without_stop,validation_input_without_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "['We had requested two queen beds and got a room with 1 queen and 2 twins, we were advised that there were not any other rooms and could put the (2) beds together, which we did and it was no issue as far as sleeping.']\n",
      "After: \n",
      "['We requested two queen beds got room 1 queen 2 twins , advised rooms could put ( 2 ) beds together , issue far sleeping .']\n",
      "\n",
      "Before: \n",
      "['Waited over 40 minutes to use shower.']\n",
      "After: \n",
      "['Waited 40 minutes use shower .']\n"
     ]
    }
   ],
   "source": [
    "s1_initial = [\"We had requested two queen beds and got a room with 1 queen and 2 twins, we were advised that there were not any other rooms and could put the (2) beds together, which we did and it was no issue as far as sleeping.\"]\n",
    "s2_initial = [\"Waited over 40 minutes to use shower.\"]\n",
    "v1,v2 = get_stop_words_removal(s1_initial,s2_initial)\n",
    "print(\"Before: \")\n",
    "print(s1_initial)\n",
    "print(\"After: \")\n",
    "print(v1)\n",
    "print()\n",
    "\n",
    "print(\"Before: \")\n",
    "print(s2_initial)\n",
    "print(\"After: \")\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\IrimieÅŸ\n",
      "[nltk_data]     Vasile\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_lemmatization(training_input, validation_input):\n",
    "    training_input_tokens = [ word_tokenize(text) for text in training_input ]\n",
    "    validation_input_tokens = [word_tokenize(text) for text in validation_input]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    training_input_stemmed_words = [[lemmatizer.lemmatize(word) for word in words] for words in training_input_tokens]\n",
    "    validation_input_stemmed_words = [[lemmatizer.lemmatize(word) for word in words] for words in validation_input_tokens]\n",
    "\n",
    "    training_input_stemmed_sentences = [' '.join(words) for words in training_input_stemmed_words]\n",
    "    validation_input_stemmed_sentences = [' '.join(words) for words in validation_input_stemmed_words]\n",
    "    \n",
    "    return training_input_stemmed_sentences,validation_input_stemmed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "['We had requested two queen beds and got a room with 1 queen and 2 twins, we were advised that there were not any other rooms and could put the (2) beds together, which we did and it was no issue as far as sleeping.']\n",
      "After: \n",
      "['We had requested two queen bed and got a room with 1 queen and 2 twin , we were advised that there were not any other room and could put the ( 2 ) bed together , which we did and it wa no issue a far a sleeping .']\n",
      "\n",
      "Before: \n",
      "['My room was immaculate and smelled so fresh and clean']\n",
      "After: \n",
      "['My room wa immaculate and smelled so fresh and clean']\n"
     ]
    }
   ],
   "source": [
    "s1_initial = [\"We had requested two queen beds and got a room with 1 queen and 2 twins, we were advised that there were not any other rooms and could put the (2) beds together, which we did and it was no issue as far as sleeping.\"]\n",
    "s2_initial = [\"My room was immaculate and smelled so fresh and clean\"]\n",
    "v1,v2 = get_lemmatization(s1_initial,s2_initial)\n",
    "print(\"Before: \")\n",
    "print(s1_initial)\n",
    "print(\"After: \")\n",
    "print(v1)\n",
    "print()\n",
    "\n",
    "print(\"Before: \")\n",
    "print(s2_initial)\n",
    "print(\"After: \")\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(training_input, number_of_clusters:int):\n",
    "    unsupervisedClassifier = KMeans(n_clusters=number_of_clusters, random_state=0)\n",
    "    unsupervisedClassifier.fit(training_input)\n",
    "    return unsupervisedClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier:KMeans,validation_input, validation_output, label_names,positive_label):\n",
    "    computedTestIndexes = classifier.predict(validation_input)\n",
    "    computed_outputs = [label_names[value] for value in computedTestIndexes]\n",
    "    TP, TN, FP, FN = get_TP_TN_FP_FN(computed_outputs,validation_output,positive_label)\n",
    "    accuracy = get_accuracy(TP, TN, FP, FN)\n",
    "    precision = get_precision(TP, TN, FP, FN)\n",
    "    recall = get_recall(TP, TN, FP, FN)\n",
    "    print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32967032967032966\n",
      "Precision: 0.3448275862068966\n",
      "Recall: 0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "dataframe = read_datas('reviews_mixed.csv')\n",
    "training_input,training_output,validation_input, validation_output = get_training_and_validation_datas(dataframe)\n",
    "test_feats, validation_feats = get_bags_of_words(training_input,validation_input)\n",
    "label_names = [name for name in set(training_output)]\n",
    "classifier = get_classifier(test_feats,len(label_names))\n",
    "test_classifier(classifier,validation_feats,validation_output,label_names,'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictia pentru mesajul:\n",
    "\n",
    " > By choosing a bike over a car, Iâ€™m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and Iâ€™m proud to be part of that movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(classifier:KMeans, label_names, text:str, training_input):\n",
    "    input = [text]\n",
    "    _, input = get_bags_of_words(training_input, input)\n",
    "    output = classifier.predict(input)\n",
    "    label = label_names[output[-1]]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: positive\n"
     ]
    }
   ],
   "source": [
    "text = \"By choosing a bike over a car, I'm reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I'm proud to be part of that movement.\"\n",
    "label = predict_text(classifier,label_names, text,training_input)\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative la k-means si analiza performanta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3225806451612903\n",
      "Precision: 0.3111111111111111\n",
      "Recall: 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "def get_classifier(training_input, number_of_clusters:int):\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=number_of_clusters)\n",
    "    dbscan.fit(training_input)\n",
    "    return dbscan\n",
    "\n",
    "def test_classifier(classifier:DBSCAN,validation_input, validation_output, label_names,positive_label):\n",
    "    computedTestIndexes = classifier.fit_predict(validation_input)\n",
    "    computed_outputs = [label_names[value] for value in computedTestIndexes]\n",
    "    TP, TN, FP, FN = get_TP_TN_FP_FN(computed_outputs,validation_output,positive_label)\n",
    "    accuracy = get_accuracy(TP, TN, FP, FN)\n",
    "    precision = get_precision(TP, TN, FP, FN)\n",
    "    recall = get_recall(TP, TN, FP, FN)\n",
    "    print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\")\n",
    "\n",
    "dataframe = read_datas('reviews_mixed.csv')\n",
    "training_input,training_output,validation_input, validation_output = get_training_and_validation_datas(dataframe)\n",
    "test_feats, validation_feats = get_bags_of_words(training_input,validation_input)\n",
    "label_names = [name for name in set(training_output)]\n",
    "classifier = get_classifier(test_feats,len(label_names))\n",
    "test_classifier(classifier,validation_feats,validation_output,label_names,'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44565217391304346\n",
      "Precision: 0.38666666666666666\n",
      "Recall: 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "def get_classifier(training_input, number_of_clusters:int):\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=number_of_clusters)\n",
    "    agg_clustering.fit(training_input.toarray())\n",
    "    return agg_clustering\n",
    "\n",
    "def test_classifier(classifier:AgglomerativeClustering,validation_input, validation_output, label_names,positive_label):\n",
    "    computedTestIndexes = classifier.fit_predict(validation_input.toarray())\n",
    "    computed_outputs = [label_names[value] for value in computedTestIndexes]\n",
    "    TP, TN, FP, FN = get_TP_TN_FP_FN(computed_outputs,validation_output,positive_label)\n",
    "    accuracy = get_accuracy(TP, TN, FP, FN)\n",
    "    precision = get_precision(TP, TN, FP, FN)\n",
    "    recall = get_recall(TP, TN, FP, FN)\n",
    "    print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\")\n",
    "\n",
    "dataframe = read_datas('reviews_mixed.csv')\n",
    "training_input,training_output,validation_input, validation_output = get_training_and_validation_datas(dataframe)\n",
    "test_feats, validation_feats = get_bags_of_words(training_input,validation_input)\n",
    "label_names = [name for name in set(training_output)]\n",
    "classifier = get_classifier(test_feats,len(label_names))\n",
    "test_classifier(classifier,validation_feats,validation_output,label_names,'positive')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
