{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <h3>k-fold cross-validation</h3>\n",
    "  <ul>\n",
    "    <li>Impartim setul de date in k sub-seturi</li>\n",
    "    <li>Apoi\n",
    "      <ul>\n",
    "        <li>Iteram de k ori</li>\n",
    "        <li>luam k-1 seturi ca set de training</li>\n",
    "        <li>restul de 1 setul de validare</li>\n",
    "        <li>antrenam regresorul</li>\n",
    "        <li>lil_err = ...</li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li>err = Suma de li_err</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <h3>Validare încrucișată leave-one-out (LOOCV)</h3>\n",
    "  <ul>\n",
    "    <li>Subcaz al lui k-fold cross validation unde k = numarul de inregistrari</li>\n",
    "    <li>Apoi\n",
    "      <ul>\n",
    "        <li>Iteram de k ori</li>\n",
    "        <li>luam k-1 seturi ca set de training</li>\n",
    "        <li>restul de 1 setul de validare</li>\n",
    "        <li>antrenam regresorul</li>\n",
    "        <li>lil_err = ...</li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li>err = Suma de li_err</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li>Mean Squared Error (MSE):\n",
    "    <ul>\n",
    "        <li>Funcția de pierdere clasică pentru problemele de regresie.</li>\n",
    "        <li>Calculează media pătratelor diferențelor între predicțiile modelului și valorile țintă.</li>\n",
    "        <li>Este sensibilă la valori extreme și poate fi afectată de outliere.</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Binary Cross-Entropy (Log Loss):\n",
    "    <ul>\n",
    "        <li>Folosită în probleme de clasificare binară, unde țintele sunt etichete binare (0 sau 1).</li>\n",
    "        <li>Măsoară discrepanța între distribuția de probabilitate prezisă de model și distribuția reală a claselor.</li>\n",
    "        <li>Este o funcție convexă și este comun utilizată în combinație cu funcția sigmoid pentru a obține probabilități între 0 și 1</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Categorical Cross-Entropy:\n",
    "    <ul>\n",
    "        <li>Similar cu binary cross-entropy, dar pentru probleme de clasificare cu mai multe clase.</li>\n",
    "        <li>Este utilizată atunci când etichetele sunt reprezentate ca variabile categoriale.</li>\n",
    "        <li>Măsoară discrepanța între distribuția de probabilitate a claselor prezise și distribuția reală a claselor.</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Hinge Loss:\n",
    "    <ul>\n",
    "        <li>Utilizată în special în problemele de clasificare cu suport vectorial (SVM).</li>\n",
    "        <li>Măsoară marginea (marginea dintre clasa corectă și celelalte clase) și penalizează predicțiile greșite care sunt aproape de margine.</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Huber Loss:\n",
    "    <ul>\n",
    "        <li>O alternativă la mean squared error care este mai robustă la valori extreme și la outliere.</li>\n",
    "        <li>Se comportă ca MSE în jurul originii și ca MAE în alte zone.</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li>Kullback-Leibler Divergence (KL Divergence):\n",
    "    <ul>\n",
    "        <li>Măsoară diferența dintre două distribuții de probabilitate.</li>\n",
    "        <li>Este adesea folosită în problemele de regresie și clasificare, în special în învățarea automată generativă.</li>\n",
    "    </ul>\n",
    "</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
